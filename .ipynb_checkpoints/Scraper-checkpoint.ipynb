{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e732b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import os.path\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d66765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling URLs to scrape\n",
    "years = list(range(2000, 2023))\n",
    "url_big12 = 'https://www.sports-reference.com/cbb/conferences/big-12/{}-stats.html'\n",
    "url_sec = 'https://www.sports-reference.com/cbb/conferences/sec/{}-stats.html'\n",
    "url_big10 = 'https://www.sports-reference.com/cbb/conferences/big-ten/{}-stats.html'\n",
    "url_bigeast = 'https://www.sports-reference.com/cbb/conferences/big-east/{}-stats.html'\n",
    "url_pac12 = 'https://www.sports-reference.com/cbb/conferences/pac-12/{}-stats.html'\n",
    "url_acc = 'https://www.sports-reference.com/cbb/conferences/acc/{}-stats.html'\n",
    "url_aac = 'https://www.sports-reference.com/cbb/conferences/aac/{}-stats.html'\n",
    "url_mwc = 'https://www.sports-reference.com/cbb/conferences/mwc/{}-stats.html'\n",
    "url_wcc = 'https://www.sports-reference.com/cbb/conferences/wcc/{}-stats.html'\n",
    "url_a10 = 'https://www.sports-reference.com/cbb/conferences/atlantic-10/{}-stats.html'\n",
    "url_mvc = 'https://www.sports-reference.com/cbb/conferences/mvc/{}-stats.html'\n",
    "url_cusa = 'https://www.sports-reference.com/cbb/conferences/cusa/{}-stats.html'\n",
    "url_col = 'https://www.sports-reference.com/cbb/conferences/colonial/{}-stats.html'\n",
    "url_south = 'https://www.sports-reference.com/cbb/conferences/southern/{}-stats.html'\n",
    "url_wac = 'https://www.sports-reference.com/cbb/conferences/wac/{}-stats.html'\n",
    "url_maac = 'https://www.sports-reference.com/cbb/conferences/maac/{}-stats.html'\n",
    "url_bw = 'https://www.sports-reference.com/cbb/conferences/big-west/{}-stats.html'\n",
    "url_sb = 'https://www.sports-reference.com/cbb/conferences/sun-belt/{}-stats.html'\n",
    "url_ivy = 'https://www.sports-reference.com/cbb/conferences/ivy/{}-stats.html'\n",
    "url_mac = 'https://www.sports-reference.com/cbb/conferences/mac/{}-stats.html'\n",
    "url_as = 'https://www.sports-reference.com/cbb/conferences/atlantic-sun/{}-stats.html'\n",
    "url_ovc = 'https://www.sports-reference.com/cbb/conferences/ovc/{}-stats.html'\n",
    "url_summit = 'https://www.sports-reference.com/cbb/conferences/summit/{}-stats.html'\n",
    "url_big_south = 'https://www.sports-reference.com/cbb/conferences/big-south/{}-stats.html'\n",
    "url_big_sky = 'https://www.sports-reference.com/cbb/conferences/big-sky/{}-stats.html'\n",
    "url_america_east = 'https://www.sports-reference.com/cbb/conferences/america-east/{}-stats.html'\n",
    "url_horizon = 'https://www.sports-reference.com/cbb/conferences/horizon/{}-stats.html'\n",
    "url_patriot = 'https://www.sports-reference.com/cbb/conferences/patriot/{}-stats.html'\n",
    "url_southland = 'https://www.sports-reference.com/cbb/conferences/southland/{}-stats.html'\n",
    "url_northeast = 'https://www.sports-reference.com/cbb/conferences/northeast/{}-stats.html'\n",
    "url_meac = 'https://www.sports-reference.com/cbb/conferences/meac/{}-stats.html'\n",
    "url_swac = 'https://www.sports-reference.com/cbb/conferences/swac/{}-stats.html'\n",
    "url_draft =     'https://www.nbadraft.net/{}-nba-draft-combine-measurements/'\n",
    "url_draft_summary = 'https://basketball.realgm.com/nba/draft/past_drafts/{}'\n",
    "\n",
    "list_of_urls = [\n",
    "url_big12, \n",
    "url_sec, \n",
    "url_big10,\n",
    "url_bigeast,\n",
    "url_pac12,\n",
    "url_acc,\n",
    "url_aac,\n",
    "url_mwc,\n",
    "url_wcc,\n",
    "url_a10,\n",
    "url_mvc,\n",
    "url_cusa,\n",
    "url_col,\n",
    "url_south,\n",
    "url_wac,\n",
    "url_maac,\n",
    "url_bw,\n",
    "url_sb,\n",
    "url_ivy,\n",
    "url_mac,\n",
    "url_as,\n",
    "url_ovc ,\n",
    "url_summit,\n",
    "url_big_south,\n",
    "url_big_sky,\n",
    "url_america_east,\n",
    "url_horizon,\n",
    "url_patriot,\n",
    "url_southland,\n",
    "url_northeast,\n",
    "url_meac,\n",
    "url_swac\n",
    "]\n",
    "#Table ID\n",
    "NCAA_id = 'conference-stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19644134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates File name\n",
    "def generate_name(url, year):\n",
    "    end_index = url[49:].find('/')\n",
    "    name = url[49:49+end_index]+'_'+str(year)\n",
    "    return(name)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f36f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates File Directory\n",
    "def generate_directory(directory, file_name):\n",
    "    generated_directory = directory + '/{}.html'.format(file_name)\n",
    "    return(generated_directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a168aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloads HTML File from given URL to directory\n",
    "def get_html_request(directory, url):\n",
    "    for year in years:\n",
    "        url_temp = url.format(year)\n",
    "        data = requests.get(url_temp)\n",
    "        with open(generate_directory(directory, generate_name(url, year)), 'w+',encoding=\"utf-8\") as f:\n",
    "                f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ee924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloads HTML File from given URL to /NCAA\n",
    "def get_html_request_NCAA(url):\n",
    "    for year in years:\n",
    "        url_temp = url.format(year)\n",
    "        data = requests.get(url_temp)\n",
    "        with open('NCAA/{}.html'.format(generate_name(url, year)), 'w+',encoding=\"utf-8\") as f:\n",
    "                f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "910f370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloads HTML File from given URL to /Draft\n",
    "def get_html_request_draft(url):\n",
    "    for year in years:\n",
    "        url_temp = url.format(year)\n",
    "        data = requests.get(url_temp)\n",
    "        with open('Draft/{}.html'.format(year), 'w+',encoding=\"utf-8\") as f:\n",
    "                f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe4780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloads HTML File from given URL to /NCAA, uses Selenium to execute javascript if required\n",
    "def get_html_api(url):\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    for year in year:        \n",
    "        temp_url = url.format(year)\n",
    "        driver.get(temp_url)\n",
    "        driver.execute_script(\"window.scrollTo(1,10000)\")\n",
    "        time.sleep(4)\n",
    "        with open(\"NCAA/{}.html\".format(generate_name(url, year)), \"w+\",encoding=\"utf-8\") as f:\n",
    "            f.write(driver.page_source)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e65da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searches webpage for all tables\n",
    "def search_url_for_tables(url):\n",
    "    r = get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    tables = soup.find_all('table')\n",
    "    return(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8877f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses requests to get html and searches for all tables, returning them as a df with first row as header\n",
    "def get_df_from_url(url):\n",
    "    r = get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    tables = soup.find_all('table')\n",
    "    df = pd.read_html(str(tables))[0]\n",
    "    new_header = df.iloc[0] #grab the first row for the header\n",
    "    df = df[1:] #take the data unless the header row\n",
    "    df.columns = new_header\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "872efe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses requests to get html and searches for all tables, returning them as a df with no header\n",
    "def get_df_from_url_noheader(url):\n",
    "    r = get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    tables = soup.find_all('table')\n",
    "    df = pd.read_html(str(tables))[0]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a31b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opens html file and extracts table to df from table id\n",
    "def html_to_df_from_tableid(file_name, table_id):\n",
    "    with open('NCAA/{}.html'.format(file_name), encoding=\"utf-8\") as f:\n",
    "        page = f.read()\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        soup.find('tr', class_= 'over_header').decompose() #Removes 'over_header' element of table\n",
    "        table = soup.find(id=table_id)\n",
    "        df = pd.read_html(str(table))[0]\n",
    "    return df\n",
    "\n",
    "#Uses URL to get df based on table id\n",
    "def html_to_df_from_tableid_no_download(url, table_id):\n",
    "    r = get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    table = soup.find(id=table_id)\n",
    "    df = pd.read_html(str(table))[0]\n",
    "    return df\n",
    "\n",
    "#Uses URL to get df based on table class\n",
    "def html_to_df_from_table_class_no_download(url):\n",
    "    r = get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    table = soup.find('table', {'class' : [\"tablesaw\", \"tablesaw-swipe\", \"tablesaw-sortable\"]})\n",
    "    df = pd.read_html(str(table))[0]\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e52bfb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes Positons from player names as to only include player name in 'Player' Collumn\n",
    "def clean_player_name(series):\n",
    "    new_series = []\n",
    "    for item in series:\n",
    "        index = 0\n",
    "        for char in reversed(item):\n",
    "            if char.capitalize() == char or char == '-':\n",
    "                index = index - 1\n",
    "            else:\n",
    "                break\n",
    "        new_series.append(item[:index].strip())\n",
    "    return(new_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b100d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts from inches to cm so model can interpret data\n",
    "def inch_to_cm(series):\n",
    "    new_series = []\n",
    "    for measurement in series:\n",
    "        measurement = measurement.strip()\n",
    "        feet = float(measurement[0])\n",
    "        inches = float(measurement[2:-1])\n",
    "        height_inches = feet*12 + inches\n",
    "        height_cm = round(height_inches * 2.54, 2)\n",
    "        new_series.append(height_cm)\n",
    "    return(new_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fac48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes instances in dataset where last character is a non-int value \n",
    "def remove_last_char(series):\n",
    "    new_series = []\n",
    "    for item in series:\n",
    "        if isinstance(item, str):\n",
    "            if item[-1] == '‘' or item[-1] == '′' or item[-1] == '%':\n",
    "                new_series.append(item[:-1])\n",
    "            else:\n",
    "                new_series.append(item)\n",
    "        else:\n",
    "            new_series.append(item)\n",
    "    return(new_series)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88937574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.static import players\n",
    "from nba_api.stats.endpoints import draftcombinestats\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "combine_dfs = []\n",
    "for year in years:\n",
    "    combine = draftcombinestats.DraftCombineStats(season_all_time = year)\n",
    "    combine_df = combine.get_data_frames()\n",
    "    \n",
    "    combine_dfs.append(combine_df[0])\n",
    "    sleep(0.6)\n",
    "combine_data_df = pd.concat(combine_dfs)\n",
    "\n",
    "combine_data_df.to_csv('combine_total.csv')         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b49ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time = 15:30 for .unique()\n",
    "\n",
    "from nba_api.stats.static import players\n",
    "from nba_api.stats.endpoints import draftcombinestats\n",
    "from nba_api.stats.endpoints import commonplayerinfo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_draft_info():\n",
    "    \n",
    "    draft_dfs = []\n",
    "    \n",
    "    player_dict = players.get_players()\n",
    "    for player in player_dict:\n",
    "        if player['full_name'] in combine_data_df['PLAYER_NAME'].unique(): #try list() and .values()\n",
    "            player_info = commonplayerinfo.CommonPlayerInfo(player_id=player['id'])\n",
    "            df = player_info.get_data_frames()\n",
    "            draft_dfs.append(df[0])\n",
    "            sleep(0.6)\n",
    "            \n",
    "    draft_data_df = pd.concat(draft_dfs)\n",
    "    important_columns = ['PERSON_ID', 'DISPLAY_FIRST_LAST', 'SCHOOL', 'BIRTHDATE', 'SEASON_EXP', 'DRAFT_YEAR', 'DRAFT_ROUND', 'DRAFT_NUMBER']\n",
    "    \n",
    "    draft_df = draft_data_df[important_columns]\n",
    "    \n",
    "    '''\n",
    "    for player in combine_data_df['PLAYER_ID']:\n",
    "        print(player)\n",
    "        draft = commonplayerinfo.CommonPlayerInfo(player_id = player)\n",
    "        draft_df = draft.get_data_frames()\n",
    "        draft_dfs.append(draft_df[0])\n",
    "        sleep(0.5)\n",
    "    draft_data_df = pd.concat(draft_dfs)\n",
    "    \n",
    "\n",
    "     \n",
    "    player_dict = players.get_players()\n",
    "    for player in player_dict:\n",
    "        if player['full_name'] == name:\n",
    "            player_info = commonplayerinfo.CommonPlayerInfo(player_id=player['id'])\n",
    "            df = player_info.get_data_frames()\n",
    "            df[1]['SCHOOL'] = df[0]['SCHOOL']\n",
    "    '''\n",
    "    \n",
    "    return(draft_df)\n",
    "    \n",
    "draft_df = get_draft_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b515921",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_rename = {'DISPLAY_FIRST_LAST': 'Player'}\n",
    "draft_df.rename(columns = columns_to_rename)\n",
    "draft_df.to_csv('draft.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9c6a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_columns = list(combine_data_df.columns)\n",
    "columns_to_drop =   ['HEIGHT_W_SHOES',\n",
    "                     'HEIGHT_W_SHOES_FT_IN',\n",
    "                     'HEIGHT_WO_SHOES_FT_IN',\n",
    "                     'WINGSPAN_FT_IN',\n",
    "                     'BODY_FAT_PCT',\n",
    "                     'HAND_LENGTH',\n",
    "                     'HAND_WIDTH',\n",
    "                     'MODIFIED_LANE_AGILITY_TIME',\n",
    "                     'SPOT_FIFTEEN_CORNER_LEFT',\n",
    "                     'SPOT_FIFTEEN_BREAK_LEFT',\n",
    "                     'SPOT_FIFTEEN_TOP_KEY',\n",
    "                     'SPOT_FIFTEEN_BREAK_RIGHT',\n",
    "                     'SPOT_FIFTEEN_CORNER_RIGHT',\n",
    "                     'SPOT_COLLEGE_CORNER_LEFT',\n",
    "                     'SPOT_COLLEGE_BREAK_LEFT',\n",
    "                     'SPOT_COLLEGE_TOP_KEY',\n",
    "                     'SPOT_COLLEGE_BREAK_RIGHT',\n",
    "                     'SPOT_COLLEGE_CORNER_RIGHT',\n",
    "                     'SPOT_NBA_CORNER_LEFT',\n",
    "                     'SPOT_NBA_BREAK_LEFT',\n",
    "                     'SPOT_NBA_TOP_KEY',\n",
    "                     'SPOT_NBA_BREAK_RIGHT',\n",
    "                     'SPOT_NBA_CORNER_RIGHT',\n",
    "                     'OFF_DRIB_FIFTEEN_BREAK_LEFT',\n",
    "                     'OFF_DRIB_FIFTEEN_TOP_KEY',\n",
    "                     'OFF_DRIB_FIFTEEN_BREAK_RIGHT',\n",
    "                     'OFF_DRIB_COLLEGE_BREAK_LEFT',\n",
    "                     'OFF_DRIB_COLLEGE_TOP_KEY',\n",
    "                     'OFF_DRIB_COLLEGE_BREAK_RIGHT',\n",
    "                     'ON_MOVE_FIFTEEN',\n",
    "                     'ON_MOVE_COLLEGE',\n",
    "                     ]                                       #2016 = none\n",
    "\n",
    "combine_df = combine_data_df.drop(columns = columns_to_drop)\n",
    "combine_df = combine_df.dropna()\n",
    "\n",
    "\n",
    "columns_to_rename = {'PLAYER_NAME': 'Player'}\n",
    "combine_df = combine_df.rename(columns = columns_to_rename)\n",
    "combine_df.to_csv('combine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81be247a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor url in list_of_urls[]:\\n    get_html_api(url)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scrapes around 200 webpages from basketball reference, Saves files in /NCAA\n",
    "#Can run it if you would like but Selenium may crash every once in a while\n",
    "#If this occurs I would suggest indexing list_of_urls to avoid scraping pages already scraped before the crash and re-running cell\n",
    "'''\n",
    "for url in list_of_urls[]:\n",
    "    get_html_api(url)\n",
    "'''\n",
    "#All files nessecary already in /NCAA directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "406a57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes through each HTML file in NCAA and extracts table as a df and adds it to the dfs_college list \n",
    "#If no table/player data in HTML, file is deleted\n",
    "#Cell will take around 5 minutes to run\n",
    "dfs_college = []\n",
    "for url in list_of_urls:\n",
    "    for year in years:\n",
    "        file_name = generate_name(url, year)\n",
    "        path = Path('./NCAA/{}.html'.format(file_name))\n",
    "        if os.path.isfile(path):                                       #Checks if file exists\n",
    "            with open('NCAA/{}.html'.format(file_name), encoding=\"utf-8\") as f:\n",
    "                page = f.read()\n",
    "                soup = BeautifulSoup(page, 'html.parser')\n",
    "                try:                                                   #Creates df from player data table and adds it to a list\n",
    "                    soup.find('tr', class_= 'over_header').decompose() #This will raise AttributeError if there is no player data on webpage\n",
    "                    table = soup.find(id=NCAA_id)                      \n",
    "                    df = pd.read_html(str(table))[0]\n",
    "                    df['Year'] = year\n",
    "                    dfs_college.append(df)\n",
    "                except AttributeError:                                 #If no player data table exists the HTML file is deleted\n",
    "                    f.close()                                          #This occurs because some conferences did not exist until later \n",
    "                    os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc0a82f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Merges df containing college player stats and NBA draft info to a single df which is written to total.csv for analysis\\ntotal = pd.merge(college_total_clean, combine_df, on=['Player', 'Year'])\\ntotal['3P%'] = total['3P%'].fillna(0)\\ntotal = total.dropna()\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adds all college players to a single df and gets relevant collumns\n",
    "college_total = pd.concat(dfs_college)\n",
    "relevant_columns = ['Player', 'Pos', 'G','MP','FG%','2P%','3P%','FT%','PTS.1','TRB.1','AST.1','STL.1','BLK.1','PER','WS','BPM', 'Year']\n",
    "columns_to_rename = {'PTS.1': 'PTS',\n",
    "                    'AST.1': 'AST',\n",
    "                    'STL.1': 'STL',\n",
    "                    'BLK.1': 'BLK',\n",
    "                    'TRB.1': 'TRB'}\n",
    "\n",
    "college_total_clean = college_total.loc[:,relevant_columns]\n",
    "college_total_clean.rename(columns = columns_to_rename , inplace = True)\n",
    "\n",
    "college_total_clean.to_csv('NCAA Stats')\n",
    "\n",
    "'''\n",
    "#Merges df containing college player stats and NBA draft info to a single df which is written to total.csv for analysis\n",
    "total = pd.merge(college_total_clean, combine_df, on=['Player', 'Year'])\n",
    "total['3P%'] = total['3P%'].fillna(0)\n",
    "total = total.dropna()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "college_total.loc[college_total['Player'] == 'Jordan Bell']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
